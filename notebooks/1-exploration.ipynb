{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AmazonZero\n",
    "\n",
    "In this notebook we'll explore on how to develop a AI for playing the game of amazons. The tactic is going to be to develop an AI using reinforcement learning. So we'll create an agent which consists of two parts:\n",
    "\n",
    "1. A predictor: A neural network which predicts the winner: black or white (categorical), given some input data: Square States (categorical), Game States (categorical), Available squares white (integer), Available square black (integer).\n",
    "2. An actor: A decision maker which monte carlo tree searches through all the possible moves and makes predictions on the board states for each deeper level. It then picks the tree which will result in the highest probability of a win for the acting side.\n",
    "\n",
    "## The input data\n",
    "\n",
    "We give AI three different sorts of input. \n",
    "\n",
    "### Square states\n",
    "There are a total of four square states: [Empty, White Amazon, Black Amazon, Fire]. Which we'll encode as follows:\n",
    "\n",
    "[E, WA, BA, F] = [0, 1, 2, 3] = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "\n",
    "This is so the network picks up that these are categories and there is no certain order in them. This part of the input is fed seperately to the network.\n",
    "\n",
    "### Board states\n",
    "There are a total of four board states: [White selects, White shoots, Black selects, Black shoots]. Which we'll encode in the same way.\n",
    "\n",
    "[Wse, Wsh, Bse, Bsh] = [0, 1, 2, 3] = [[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "\n",
    "And again we will feed this in to the network seperately.\n",
    "\n",
    "### Available squares white\n",
    "We will also pass in a single node which contains the information about the number of available square for white. This value will be scaled between 0 and 1, and will be fed into the network seperately. \n",
    "\n",
    "### Available square black\n",
    "We will also pass in a single node which contains the informat about the number of available squares for black. This value will be scaled betwee 0 and 1, and will be fed into the network seperately.\n",
    "\n",
    "## The output data\n",
    "\n",
    "The network will try to predict wether white will win or black will win.\n",
    "\n",
    "White wins: [W] = [1] = [1, 0]\n",
    "Black wins: [B] = [0] = [0, 1]\n",
    "\n",
    "# Plan de Campagne\n",
    "\n",
    "1. Random AI: We will create an AI which will play random moves.\n",
    "- We still need to implement game end\n",
    "- We need to implement the player on the server.\n",
    "\n",
    "\n",
    "2. Smart AI: We will create a smart AI which monte carlo tree searches all possible moves and picks the path which will result in the most possible moves for the acting side\n",
    "\n",
    "3. Neural Net AI: We will create two AI with an actor and predictor and let them play lots of games against eachother. After a certain amount of games we will update the nodes with the results of all the games in memory. NOTE: This AI uses a exploration and exploitation to explore. Where in the beginning there will be lots of exploration and once it has learned from a lot of previous games the AI will use more and more exploitation.\n",
    "\n",
    "https://www.youtube.com/watch?v=RfNxXlO6BiA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from amazonzero.board import Board\n",
    "from amazonzero.player import RandomPlayer, ForestPlayer, Actor, Predictor\n",
    "from amazonzero.main import play_a_game\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "TURN = ['white', 'black']\n",
    "\n",
    "colormap = {'white': 1, 'black': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 2., 0.],\n",
       "       [0., 0., 0., 0., 0., 2.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = Board(6, 2)\n",
    "p1 = RandomPlayer(board, 'white')\n",
    "p2 = RandomPlayer(board, 'black')\n",
    "\n",
    "board.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3. 3. 3. 3. 1.]\n",
      " [0. 3. 3. 3. 3. 3.]\n",
      " [0. 3. 1. 3. 3. 2.]\n",
      " [0. 3. 0. 3. 2. 3.]\n",
      " [3. 3. 3. 3. 3. 3.]\n",
      " [3. 0. 3. 3. 3. 3.]]\n",
      "Winner is white\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADQklEQVR4nO3dMW7bQBBAUW6gSjlGcv8T+Rp2OynSxDFFwIDh/TbeK7nNYKWPAdhwzcwB9PzYPQBwTpwQJU6IEidEiROibpena3mVy7fz8vy0e4RX7vdf6+y5zQlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtSamavzy8NPt9buCeDjzZz+sW1OiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRt90DfHUvz0+7R3jl/vP37hHy1u4B/jMPntucECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidErZlHX6Q/juPx5+r3WGv3BG9d39/ni91Ra5q/Yr/YccycXpPNCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFC1O3ydJ1+qp5/uaNLs3uAL8zmhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQdds9wLvM7J7grbV2T8A3ZXNClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBC1Zmb3DMAJmxOixAlR4oQocUKUOCFKnBD1B4PCIcpEwf5qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "play_a_game(board, p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main loop to fill a Predictor memory with random games.\n",
    "n_generations = 8\n",
    "n_games = 30\n",
    "clf = RandomForestClassifier(max_depth=10, criterion='entropy', random_state=0, n_jobs=8)\n",
    "predictor = Predictor(clf, max_mem=100_000)\n",
    "epsilon = 0.5  # Explore vs Exploit\n",
    "fitted = False\n",
    "huge_mem = np.zeros((1_000_000, 37))\n",
    "huge_mem_size = 0\n",
    "\n",
    "for gen in range(n_generations):\n",
    "    predictor = Predictor(predictor.model, max_mem=100_000)\n",
    "    actor = Actor()\n",
    "    \n",
    "    for i in tqdm.trange(n_games, leave=True):\n",
    "        board_states = []\n",
    "        board = Board(6, 2)\n",
    "\n",
    "        while not board.check_if_ended():\n",
    "            # Explore\n",
    "            if random.uniform(0,1) < epsilon or fitted == False:\n",
    "                next_board = actor.random_move(board)\n",
    "            # Exploit\n",
    "            else:\n",
    "                next_board = actor.move(board, predictor)\n",
    "            board.matrix = next_board\n",
    "            board_states.append(next_board.flatten())\n",
    "            \n",
    "        board_states = np.array(board_states)\n",
    "\n",
    "        # append winner to the last column\n",
    "        n_moves = len(board_states)\n",
    "        y = np.zeros(n_moves) + 1 if board.winner == \"white\" else np.zeros(n_moves) + 2\n",
    "\n",
    "        # create the feature matrix\n",
    "        feature_matrix = np.concatenate([board_states, y.reshape(1, -1).T], axis=1)\n",
    "        predictor.memorize(feature_matrix)\n",
    "        \n",
    "    predictor.train()\n",
    "    fitted = True\n",
    "    mem_len = predictor.mem_size\n",
    "    huge_mem[huge_mem_size:huge_mem_size+mem_len, :] = predictor.memory[:mem_len, :]\n",
    "    huge_mem_size += mem_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#create a pickle file\n",
    "with open(\"../models/rfg2_500pg.pkl\", 'wb') as picklefile:\n",
    "    pickle.dump(predictor.model, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/rfg2_500pg.pkl\", 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.00it/s]\n"
     ]
    }
   ],
   "source": [
    "board = Board(6, 2)\n",
    "p1 = RandomPlayer(actor)\n",
    "p2 = ForestPlayer(predictor, actor)\n",
    "\n",
    "results = []\n",
    "for i in tqdm.trange(100):\n",
    "    board = Board(6, 2)\n",
    "    result = play_a_game(board, p1, p2)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(results) == 'black')/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 generatie (500 games): 74% op wit, 45% op zwart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
